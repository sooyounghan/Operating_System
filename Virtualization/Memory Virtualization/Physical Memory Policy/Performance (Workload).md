-----
### 워크로드에 따른 성능 비교
-----
1. 지역성이 없는 경우
   - 접근되는 페이지들의 집합에서 페이지가 무작위적으로 참고되는 것을 의미
   - 100개의 페이지들이 일정 시간 동안 계속 접근하는 워크로드를 사용
   - 접근되는 페이지는 무작위적으로 선택되며 페이지들이 총 10,000번 접근됨
   - 캐시의 크기를 매우 작은 것부터(한 페이지) 모든 페이지들을 담을 수 있을 정도의 크기까지(100페이지) 증가시켰으며, 각 정책이 캐시 크기에 어떻게 달라지는지 확인
   - 최적 방법과 LRU, 랜덤 그리고 FIFO 방식을 사용하였을 때 실험 결과
<div align="center">
<img src="https://github.com/user-attachments/assets/42f1a4ad-52a0-4156-b806-187997602735">
</div>

   - 그림의 y축은 각 정책이 달성한 히트율, x축은 캐시 크기의 변화
   - 결론
     + 워크로드에 지역성이 없다면 어느 정책을 사용하든 상관이 없음
     + LRU와 FIFO 그리고 무작위 선택 정책은 모두 동일한 성능을 보이며, 히트율은 정확히 캐시 크기에 의해 결정
     + 캐시가 충분히 커서 모든 워크로드를 다 포함할 수 있다면 역시 어느 정책을 사용하든 상관 없음 : 참조되는 모든 블럭들이 캐시에 들어갈 수 있으면 모든 정책(무작위 포함) 히트율이 100%에 도달
     + 최적 기법이 구현 가능한 기타 정책들보다 눈에 띄게 더 좋은 성능을 보임

2. 80대 20대 워크로드 : 20%의 페이지들에서 (인기 있는 페이지) 80%의 참조가 발생하고, 80% 페이지에서 20%에 참조(비인기 페이지) 발생
   - 총 100개의 페이지가 들어있으며, 인기있는 페이지들에 대한 참조가 실행 시간의 대부분 참조
<div align="center">
<img src="https://github.com/user-attachments/assets/3dceee11-3491-4401-b12a-214880bc2090">
</div>

   - 랜덤과 FIFO 정책이 상당히 좋은 성능을 보이지만, 인기 있는 페이지들을 캐시에 더 오래두는 경향이 이는 LRU가 더 좋은 성능을 보임
   - 인기 있는 페이지들이 과거에 빈번하게 참조되었으므로 그 페이지들은 가까운 미래에 다시 참조되는 경향이 있기 때문임
   - 최적 기법은 여전히 더 좋은 성능을 보이고 있으며, 이는 LRU의 과거 정보가 완벽하지는 않다는 것을 보여줌

3. 순차 반복 워크로드 : 이 워크로드는 50개 페이지들을 순차적으로 참조 (0번 페이지 참조 후, 1번 페이지 참조, ... , 49번쨰 페이지 참조 후 다시 처음으로 돌아가서 그 접근 순서 반복)
   - 50개의 개별적인 페이지들을 총 10,000번 접근
   - 각 정책들에 대한 성능
<div align="center">
<img src="https://github.com/user-attachments/assets/a7b4ace0-3514-4e2e-8791-0ca7f2b09062">
</div>

   - 여러 응용 프로그램에 흔하게 볼 수 있음(데이터 베이스와 같은 상업용 응용 프로그램 등 포함)
   - LRU와 FIFO 정책이 가장 안 좋은 성능을 보임 : 순차 반복 워크로드에서 이 알고리즘들은 오래된 페이지를 내보내며, 워크로드의 반복적 특성으로 오래된 페이지들은 정책들이 캐시에서 유지하려고 선택한 페이지보다 먼저 접근
   - 실제로, 캐시 크기가 49라고 할지라도 50개의 페이지들을 순차 반복하는 워크로드에서 캐시 히트율은 0%가 됨
   - 무작위 선택 정책은 최적의 경우에 못 미치지만, 눈에 띄게 좋은 성능을 보임
   - 무작위 선택 정책의 히트율은 최소한 0%는 아님
