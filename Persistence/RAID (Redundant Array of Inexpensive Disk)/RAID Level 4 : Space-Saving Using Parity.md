-----
### RAID 레벨 4 : 패리티를 이용한 공간 절약
-----
1. 패리티라고 알려진 중복성을 추가하는 다른 방법
   - 패리티 기반 접근 방법은 저장 공간을 더 적게 사용하려고 하여 미러링 기반 시스템이 지불하는 엄청난 공간 낭비를 극복하려고 시도 (대신 성능이라는 비용 지불)
   - RAID-4 시스템 예
<div align="center">
<img src="https://github.com/user-attachments/assets/9f9ab798-716e-4869-af46-094c6c5bb728">
</div>

   - 각 데이터 스트라이프마다 해당 스트라이프에 대한 중복 정보를 담고 있는 패리티 블럭 하나를 추가
     + 블럭 P1은 블럭 4, 5, 6, 7번으로부터 계산된 중복 정보를 가지고 있음

 2.패리티를 계산하기 위해 스트라이프에 속해 있는 블럭 중 하나의 블럭이 고장나더라도 견딜 수 있는 수학 함수 필요 : XOR 함수 이용
   - 어떤 비트가 주어졌을 때, XOR은 1이 짝수 일때는 0을 반환하고, 홀수 개라면 0을 반환
<div align="center">
<img src="https://github.com/user-attachments/assets/956ec369-acc0-4744-97c2-016896d4a116">
</div>

   - 첫째 줄(0, 0, 1, 1)에는 C2와 C3에 1이 있음 : 그러므로 이 값들을 XOR하면 P로 0을 얻음
   - 두번째 줄에는 1이 C1에만 있음 : XOR를 하면 P는 1
   - 어떤 줄이던 그 줄의 1의 수는 짝수(홀수가 아님)가 되어야 함 : RAID 패리티가 정확하게 동작하기 위해 반드시 유지해야하는 불변량

3. 고장난 디스크로부터 데이터를 복구하는 방법
   - C2열이 깨졌다고 가정 : 그 열에 있었던 값을 알아보기 위해 단순하게 그 행의 모든 값(XOR값의 결과로 얻은 패리티 포함)을 읽은 후 올바른 값을 다시 계산
   - 구체적으로 첫 번째 행의 C2 열의 값(1)이 깨졌다고 가정
     + 같은 행의 다른 값(C0에서 0을, C1에서 0을, C3에서 1을 읽고 그리고 P열에서 패리티 0을 읽음)을 읽어서 0, 0, 1 그리고 0의 값을 얻어냄
     + 각 행의 값들을 XOR하면 해당 행에 1의 짝수 개가 되도록 해야한다는 것을 알기 때문에, 사라진 데이터의 값은 1이 되어야 함 (XOR 기반의 패리티 기법에서 값을 복구하는 방법)
   - 값을 복구하기 위해서는 계산할 때 패리티 비트와 데이터 비트를 같이 XOR한 것을 유의하며, 최초에 패리티 비트를 계산할 때 모든 비트를 사용하여 XOR했던 것과 같은 ㄴ계산
  
4. 각 디스크의 4KB(또는 그 이상)의 블럭을 저장한다면 패리티를 계산하기 위해 여러 개 블럭들을 XOR하는 방법
   - 데이터 블럭들의 비트들에 대해 비트 단위로 XOR
   - 그리고 각 비트 연산으로 XOR한 각 결과를 패리티 블럭의 해당 비트에 위치에 저장하면 됨
   - 예를 들어, 블럭이 4비트 크기를 갖고 있다면 (4KB 블럭보다 한참 작음) 다음과 같이 표현
<div align="center">
<img src="https://github.com/user-attachments/assets/21553c41-3c84-405d-af27-feca7afa484c">
</div>

   - 패리티는 각 블럭의 각 비트들에 대해 수행이 되었으며, 결과는 패리티 블럭의 해당 위치에 나타나있음

5. RAID-4 분석
   - 용량 측면 : RAID-4를 살펴보면 패리티 정보 저장을 위해 각 디스크 그룹에 속한 디스크 하나를 사용하기 위해 (N - 1) * B의 저장 공간 제공
   - 신뢰성 측면 : RAID-4는 오직 하나만의 디스크 고장을 감내할 수 있으며, 두 개 이상의 디스크가 고장나면 읽어 버린 데이터를 복원할 수 있는 방법이 없음
   - 성능 측면
     + 순차 읽기 성능의 경우 패리티 디스크를 제외한 모든 디스크를 활용할 수 있으므로 최대 유효 대역폭은 (N - 1) * S MB/s
     + 순차 쓰기 성능에 대해서는 큰 청크의 데이터를 디스크에 쓰려고 할 때, RAID-4는 스트라이프 전부 쓰기(Full-Stripe Write)라고 하는 간단한 최적화 방법 수행 가능 (예를 들어, 쓰기 요청의 일부로 블럭 0, 1, 2, 3번이 RAID로 보내진 경우)
     + 이 경우에 RAID는 P0의 새로운 값을 계산하기 위해 단순히 블럭 0, 1, 2, 3번을 XOR하면 되고, 패리티 블럭을 포함한 모든 블럭을 다섯 개의 디스크에 병렬적으로 쓰면 되므로, 스트라이프 전부 쓰기가 RAID-4에서 쓰기 작업을 하는 가장 효율적 방법
<div align="center">
<img src="https://github.com/user-attachments/assets/5d5ed89c-6947-43dc-8c46-2c10b2a600a9">
</div>

   - RAID-의 순차 쓰기 성능은 따라서 유효 대역폭이 (N - 1) * S MB/s가 되며, 패리티 디스크가 지속적으로 사용되고 있긴 하지만, 클라이언트는 그 디스크로부터 성능의 이득을 얻을 수 없음
   - 랜덤 읽기의 성능의 경우, 모든 데이터 디스크에서 랜덤 읽기로 한 블럭을 읽을 수 있지만, 패리디 디스크에서는 읽지 못하므로 유효 성능은 (N - 1) * R MB/s
   - 랜덤 쓰기의 경우에는 블럭 1번을 갱신하려는 경우를 생각
     + 패리티 블럭 P0은 더 이상 해당 스트라이프의 정확한 패리티 값을 반영하지 못하므로, P0 역시 갱신되어야 함
     + 가산적 패리티(Additive Parity) 이용 : 새로운 패리티 블럭 값을 계산하기 위해 스트라이프 내 다른 모든 데이터 블럭을 병렬적으로 읽고(예제에서는 블럭 0, 2, 3번) 새로운 블럭 1과 함께 XOR를 하며, 쓰기를 완료하기 위해 새 데이터와 새 패리티를 해당 디스크에 병렬적으로 씀
     + 하지만, 이 기법의 문제는 디스크 개수에 따라 계산 양이 달라지므로 더큰 RAID의 경우 패리티 계산을 위해 많은 수의 읽기 연산 필요
     + 감산적 패리티(Substractive Parity 이용 : 다음과 같은 비트가 나열되었다고 가정 (4개의 데이터 비트와 1개의 패리티 비트)
<div align="center">
<img src="https://github.com/user-attachments/assets/68a4650b-658c-44e3-a28e-a257d0430a9a">
</div>

   - C2 비트를 새로운 값인 $C2_{new}$로 갯신하고 싶은 경우
     + 감산적 방식은 세 단계로 동작
     + 먼저 $C2_{new}(C2_{old} = 1)$와 패리티 ($P_{old} = 1$)의 옛날 값을 각각 읽어들임
     + 그리고 엣날 값과 새로운 값을 서로 비교해 동일하다면, ($C2_{new} = C2_{old}$) 패리티 값을 그대로 유지할 것이라는 것을 암 ($P_{old} = P_{new}$)
     + 그러나 만약 서로 다르면, 이전의 패리티 비트를 현재 상태의 반대 값으로 뒤집어야 함, 즉 $P_{old} = 1$이었다면, $P_{new}$는 0이 되고, $P_{old} = 0$이었다면, $P_{new} = 1$
     + 이 복잡한 과정을 XOR로 깔끔하게 정리 가능 (⊕ : XOR 연산자)
<div align="center">
<img src="https://github.com/user-attachments/assets/5a81989a-eb03-44c4-8dfd-8ad0b5dfea19">
</div>

   - 비트가 아닌 블럭을 다루고 있으므로, 블럭의 모든 비트들에 대해 수행되어야 함 (예) 각 블럭은 4096 Byte이며 Byte당 8 Bit를 곱한 만큼 수행)
   - 대부분의 경우 새로운 블럭은 예전 블럭과 다를 것이므로 새로운 패리티 블럭 역시 다를 것
   - 가산적 패리티와 감산적 패리티를 사용하는 시기
     + 가산적 패리티 방식이 감산적 패리티 방식보다 더 적은 수의 I/O 연산을 하려면 몇 개의 디스크가 필요한지 확인하는 예제
<div align="center">
<img src="https://github.com/user-attachments/assets/7f1a35dd-a6e8-4e9f-89ae-497e1973e4ac">
</div>

   - 블럭 4번과 13번을 갱신하는 두 개의 작은 크기의 쓰기가 RAID-4로 거의 동시에 요청되었다고 가정
     + 해당 데이터는 디스크 0과 1번에 있으며, 읽기와 쓰기는 병렬적으로 일어날 수 있음
     + 패리디 디스크는 이러한 유형의 워크로드가 들어오면 병목으로 작용 : 이와 같은 경우를 패리티 기반의 RAID의 Small-Write 문제
     + 데이터 디스크가 병렬적으로 접근될 수 있다할지라도, 패리디 디스크는 어떠한 병렬 연산을 수행할 수 없음
     + 시스템에서 모든 쓰기 요청은 패리디 디스크로 인해 순차적으로 처리되고, 패리디 디스크는 논리 I/O 당 읽기 1회와 쓰기 1회의 두 번의 I/O를 처리해야 하므로 RAID-4에서 Small Random Write의 성능을 패리티 디스크에서 발생하는 두 개의 I/O 처리 성능을 계산하여 얻을 수 있음
     + 결과적으로 (R / 2) MB/s를 얻게 되며, 이 환경에서 RAID-4 성능은 매우 나쁨 (디스크를 시스템에 추가한다고해도 서능이 나아지지 않음)

   - RAID-4의 I/O 지연 시간을 분석
     + 고정이 없다고 가정하면, 읽기 요청 하나는 하나의 디스크로 전달되므로 한 개의 읽기 요청의 지연 시간은 하나의 디스크 요청 지연 시간과 같음
     + 쓰기 요청 한 개의 지연 시간은, 읽기 2번과쓰기 2번이 필요 : 읽기와 쓰기는 병렬적으로 처리될 수 있으므로 전체 지연 시간은 한 개의 디스크 요청의 두 배가 됨(두 개의 읽기가 끝나기를 기다려야 하므로, 최악의 위치 잡기 시간이 걸리지만, 대신 갱신은 탐색 비용이 들지 않으므로 평균 위치 잡기 비용보다 좋을 수 있으므로 정확히 두 배는 아님)
